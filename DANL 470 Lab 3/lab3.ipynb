{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joshua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Joshua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression is 83.798883 percent accurate\n",
      "KNN is 75.977654 percent accurate\n",
      "Naive Bayes is 82.681564 percent accurate\n",
      "Linear SVMs is 83.798883 percent accurate\n",
      "Non Linear SVMs is 74.301676 percent accurate\n",
      "Decision Trees is 81.005587 percent accurate\n",
      "Random Forests is 84.916201 percent accurate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "titanic=pd.read_csv(\"train.csv\")\n",
    "titanic.head()\n",
    "import numpy as np\n",
    "titanic_cat = titanic.select_dtypes(object)\n",
    "titanic_num = titanic.select_dtypes(np.number)\n",
    "titanic_cat.head()\n",
    "titanic_num.head()\n",
    "titanic_cat.drop(['Name','Ticket'], axis=1, inplace=True)\n",
    "titanic_cat.head()\n",
    "titanic_cat.isnull().sum()\n",
    "titanic_cat.Cabin.fillna(titanic_cat.Cabin.value_counts().idxmax(), inplace=True)\n",
    "titanic_cat.Embarked.fillna(titanic_cat.Embarked.value_counts().idxmax(), inplace=True)\n",
    "titanic_cat.isnull().sum()\n",
    "titanic_cat.head(20)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "titanic_cat = titanic_cat.apply(le.fit_transform)\n",
    "titanic_cat.head()\n",
    "titanic_num.isna().sum()\n",
    "titanic_num.Age.fillna(titanic_num.Age.mean(), inplace=True)\n",
    "titanic_num.isna().sum()\n",
    "titanic_num.drop(['PassengerId'], axis=1, inplace=True)\n",
    "titanic_num.head()\n",
    "titanic_final = pd.concat([titanic_cat,titanic_num],axis=1)\n",
    "titanic_final.head()\n",
    "X = titanic_final.drop(['Survived'],axis=1)\n",
    "Y = titanic_final['Survived']\n",
    "X_train = np.array(X[0:int(0.80*len(X))])\n",
    "Y_train = np.array(Y[0:int(0.80*len(Y))])\n",
    "X_test = np.array(X[int(0.80*len(X)):])\n",
    "Y_test = np.array(Y[int(0.80*len(Y)):])\n",
    "len(X_train), len(Y_train), len(X_test), len(Y_test)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "LR = LogisticRegression()\n",
    "KNN = KNeighborsClassifier()\n",
    "NB = GaussianNB()\n",
    "LSVM = LinearSVC()\n",
    "NLSVM = SVC(kernel='rbf')\n",
    "DT = DecisionTreeClassifier()\n",
    "RF = RandomForestClassifier()\n",
    "LR_fit = LR.fit(X_train, Y_train)\n",
    "KNN_fit = KNN.fit(X_train, Y_train)\n",
    "NB_fit = NB.fit(X_train, Y_train)\n",
    "LSVM_fit = LSVM.fit(X_train, Y_train)\n",
    "NLSVM_fit = NLSVM.fit(X_train, Y_train)\n",
    "DT_fit = DT.fit(X_train, Y_train)\n",
    "RF_fit = RF.fit(X_train, Y_train)\n",
    "LR_pred = LR_fit.predict(X_test)\n",
    "KNN_pred = KNN_fit.predict(X_test)\n",
    "NB_pred = NB_fit.predict(X_test)\n",
    "LSVM_pred = LSVM_fit.predict(X_test)\n",
    "NLSVM_pred = NLSVM_fit.predict(X_test)\n",
    "DT_pred = DT_fit.predict(X_test)\n",
    "RF_pred = RF_fit.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Logistic Regression is %f percent accurate\" % (accuracy_score(LR_pred, Y_test)*100))\n",
    "print(\"KNN is %f percent accurate\" % (accuracy_score(KNN_pred, Y_test)*100))\n",
    "print(\"Naive Bayes is %f percent accurate\" % (accuracy_score(NB_pred, Y_test)*100))\n",
    "print(\"Linear SVMs is %f percent accurate\" % (accuracy_score(LSVM_pred, Y_test)*100))\n",
    "print(\"Non Linear SVMs is %f percent accurate\" % (accuracy_score(NLSVM_pred, Y_test)*100))\n",
    "print(\"Decision Trees is %f percent accurate\" % (accuracy_score(DT_pred, Y_test)*100))\n",
    "print(\"Random Forests is %f percent accurate\" % (accuracy_score(RF_pred, Y_test)*100))\n",
    "LogisticRegression(solver='lbfgs',max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joshua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Joshua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Joshua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Joshua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Joshua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score in each iteration: [0.8156424581005587, 0.8100558659217877, 0.8100558659217877, 0.7597765363128491, 0.8491620111731844]\n",
      "K-Fold Score: 0.8089385474860336\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "titanic=pd.read_csv(\"train.csv\")\n",
    "titanic.head()\n",
    "import numpy as np\n",
    "titanic_cat = titanic.select_dtypes(object)\n",
    "titanic_num = titanic.select_dtypes(np.number)\n",
    "titanic_cat.head()\n",
    "titanic_num.head()\n",
    "titanic_cat.drop(['Name','Ticket'], axis=1, inplace=True)\n",
    "titanic_cat.head()\n",
    "titanic_cat.isnull().sum()\n",
    "titanic_cat.Cabin.fillna(titanic_cat.Cabin.value_counts().idxmax(), inplace=True)\n",
    "titanic_cat.Embarked.fillna(titanic_cat.Embarked.value_counts().idxmax(), inplace=True)\n",
    "titanic_cat.isnull().sum()\n",
    "titanic_cat.head(20)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "titanic_cat = titanic_cat.apply(le.fit_transform)\n",
    "titanic_cat.head()\n",
    "titanic_num.isna().sum()\n",
    "titanic_num.Age.fillna(titanic_num.Age.mean(), inplace=True)\n",
    "titanic_num.isna().sum()\n",
    "titanic_num.drop(['PassengerId'], axis=1, inplace=True)\n",
    "titanic_num.head()\n",
    "titanic_final = pd.concat([titanic_cat,titanic_num],axis=1)\n",
    "titanic_final.head()\n",
    "x = titanic_final.drop(['Survived'],axis=1)\n",
    "y = titanic_final['Survived']\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "seed=0\n",
    "cv = KFold(n_splits=5,random_state=2, shuffle=True)\n",
    "def return_score(model, x_train, x_test, y_train, y_test):\n",
    "    model.fit(x_train, y_train)\n",
    "    score = model.score(x_test, y_test)\n",
    "    return score\n",
    "scores = []\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "for train_index, test_index in cv.split(x,y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
    "    score = return_score(model,x_train, x_test, y_train, y_test)\n",
    "    scores.append(score)\n",
    "print(\"Accuracy score in each iteration: {}\".format(scores))\n",
    "print(\"K-Fold Score: {}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score in each iteration: [0.8156424581005587, 0.8100558659217877, 0.8100558659217877, 0.7597765363128491, 0.8491620111731844, 0.6424581005586593, 0.7541899441340782, 0.7374301675977654, 0.6815642458100558, 0.7486033519553073]\n",
      "K-Fold Score: 0.7608938547486034\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNN = KNeighborsClassifier()\n",
    "KNN_fit = KNN.fit(X_train, Y_train)\n",
    "model = KNeighborsClassifier()\n",
    "for train_index, test_index in cv.split(x,y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
    "    score = return_score(model,x_train, x_test, y_train, y_test)\n",
    "    scores.append(score)\n",
    "print(\"Accuracy score in each iteration: {}\".format(scores))\n",
    "print(\"K-Fold Score: {}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score in each iteration: [0.8156424581005587, 0.8100558659217877, 0.8100558659217877, 0.7597765363128491, 0.8491620111731844, 0.6424581005586593, 0.7541899441340782, 0.7374301675977654, 0.6815642458100558, 0.7486033519553073, 0.7597765363128491, 0.8212290502793296, 0.8212290502793296, 0.7541899441340782, 0.7374301675977654]\n",
      "K-Fold Score: 0.766852886405959\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "for train_index, test_index in cv.split(x,y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
    "    score = return_score(model,x_train, x_test, y_train, y_test)\n",
    "    scores.append(score)\n",
    "print(\"Accuracy score in each iteration: {}\".format(scores))\n",
    "print(\"K-Fold Score: {}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joshua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Joshua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Joshua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Joshua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score in each iteration: [0.8156424581005587, 0.8100558659217877, 0.8100558659217877, 0.7597765363128491, 0.8491620111731844, 0.6424581005586593, 0.7541899441340782, 0.7374301675977654, 0.6815642458100558, 0.7486033519553073, 0.7597765363128491, 0.8212290502793296, 0.8212290502793296, 0.7541899441340782, 0.7374301675977654, 0.6983240223463687, 0.6927374301675978, 0.8100558659217877, 0.7430167597765364, 0.7318435754189944]\n",
      "K-Fold Score: 0.7589385474860335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joshua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "model = LinearSVC()\n",
    "for train_index, test_index in cv.split(x,y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
    "    score = return_score(model,x_train, x_test, y_train, y_test)\n",
    "    scores.append(score)\n",
    "print(\"Accuracy score in each iteration: {}\".format(scores))\n",
    "print(\"K-Fold Score: {}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score in each iteration: [0.8156424581005587, 0.8100558659217877, 0.8100558659217877, 0.7597765363128491, 0.8491620111731844, 0.6424581005586593, 0.7541899441340782, 0.7374301675977654, 0.6815642458100558, 0.7486033519553073, 0.7597765363128491, 0.8212290502793296, 0.8212290502793296, 0.7541899441340782, 0.7374301675977654, 0.6983240223463687, 0.6927374301675978, 0.8100558659217877, 0.7430167597765364, 0.7318435754189944, 0.6815642458100558, 0.6871508379888268, 0.6927374301675978, 0.7318435754189944, 0.6703910614525139]\n",
      "K-Fold Score: 0.7456983240223463\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC()\n",
    "for train_index, test_index in cv.split(x,y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
    "    score = return_score(model,x_train, x_test, y_train, y_test)\n",
    "    scores.append(score)\n",
    "print(\"Accuracy score in each iteration: {}\".format(scores))\n",
    "print(\"K-Fold Score: {}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score in each iteration: [0.8156424581005587, 0.8100558659217877, 0.8100558659217877, 0.7597765363128491, 0.8491620111731844, 0.6424581005586593, 0.7541899441340782, 0.7374301675977654, 0.6815642458100558, 0.7486033519553073, 0.7597765363128491, 0.8212290502793296, 0.8212290502793296, 0.7541899441340782, 0.7374301675977654, 0.6983240223463687, 0.6927374301675978, 0.8100558659217877, 0.7430167597765364, 0.7318435754189944, 0.6815642458100558, 0.6871508379888268, 0.6927374301675978, 0.7318435754189944, 0.6703910614525139, 0.7877094972067039, 0.8156424581005587, 0.8044692737430168, 0.7597765363128491, 0.7653631284916201]\n",
      "K-Fold Score: 0.7525139664804469\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "for train_index, test_index in cv.split(x,y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
    "    score = return_score(model,x_train, x_test, y_train, y_test)\n",
    "    scores.append(score)\n",
    "print(\"Accuracy score in each iteration: {}\".format(scores))\n",
    "print(\"K-Fold Score: {}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score in each iteration: [0.8156424581005587, 0.8100558659217877, 0.8100558659217877, 0.7597765363128491, 0.8491620111731844, 0.6424581005586593, 0.7541899441340782, 0.7374301675977654, 0.6815642458100558, 0.7486033519553073, 0.7597765363128491, 0.8212290502793296, 0.8212290502793296, 0.7541899441340782, 0.7374301675977654, 0.6983240223463687, 0.6927374301675978, 0.8100558659217877, 0.7430167597765364, 0.7318435754189944, 0.6815642458100558, 0.6871508379888268, 0.6927374301675978, 0.7318435754189944, 0.6703910614525139, 0.7877094972067039, 0.8156424581005587, 0.8044692737430168, 0.7597765363128491, 0.7653631284916201, 0.8379888268156425, 0.8268156424581006, 0.8044692737430168, 0.8044692737430168, 0.7597765363128491]\n",
      "K-Fold Score: 0.7602553870710296\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "for train_index, test_index in cv.split(x,y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
    "    score = return_score(model,x_train, x_test, y_train, y_test)\n",
    "    scores.append(score)\n",
    "print(\"Accuracy score in each iteration: {}\".format(scores))\n",
    "print(\"K-Fold Score: {}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joshua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Joshua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Joshua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Joshua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Joshua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score in each iteration: [0.8268156424581006, 0.8491620111731844, 0.8379888268156425, 0.770949720670391, 0.8156424581005587]\n",
      "K-Fold Score: 0.8201117318435754\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "seed=0\n",
    "cv = StratifiedKFold(n_splits=5,random_state=2, shuffle=True)\n",
    "def return_score(model, x_train, x_test, y_train, y_test):\n",
    "    model.fit(x_train, y_train)\n",
    "    score = model.score(x_test, y_test)\n",
    "    return score\n",
    "scores = []\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "for train_index, test_index in cv.split(x,y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
    "    score = return_score(model,x_train, x_test, y_train, y_test)\n",
    "    scores.append(score)\n",
    "print(\"Accuracy score in each iteration: {}\".format(scores))\n",
    "print(\"K-Fold Score: {}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score in each iteration: [0.8268156424581006, 0.8491620111731844, 0.8379888268156425, 0.770949720670391, 0.8156424581005587, 0.7597765363128491, 0.7597765363128491, 0.6871508379888268, 0.7039106145251397, 0.6815642458100558]\n",
      "K-Fold Score: 0.7692737430167598\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier()\n",
    "for train_index, test_index in cv.split(x,y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
    "    score = return_score(model,x_train, x_test, y_train, y_test)\n",
    "    scores.append(score)\n",
    "print(\"Accuracy score in each iteration: {}\".format(scores))\n",
    "print(\"K-Fold Score: {}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score in each iteration: [0.8268156424581006, 0.8491620111731844, 0.8379888268156425, 0.770949720670391, 0.8156424581005587, 0.7597765363128491, 0.7597765363128491, 0.6871508379888268, 0.7039106145251397, 0.6815642458100558, 0.8044692737430168, 0.8156424581005587, 0.7597765363128491, 0.770949720670391, 0.7597765363128491]\n",
      "K-Fold Score: 0.7735567970204841\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "for train_index, test_index in cv.split(x,y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
    "    score = return_score(model,x_train, x_test, y_train, y_test)\n",
    "    scores.append(score)\n",
    "print(\"Accuracy score in each iteration: {}\".format(scores))\n",
    "print(\"K-Fold Score: {}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joshua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Joshua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Joshua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Joshua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score in each iteration: [0.8268156424581006, 0.8491620111731844, 0.8379888268156425, 0.770949720670391, 0.8156424581005587, 0.7597765363128491, 0.7597765363128491, 0.6871508379888268, 0.7039106145251397, 0.6815642458100558, 0.8044692737430168, 0.8156424581005587, 0.7597765363128491, 0.770949720670391, 0.7597765363128491, 0.7150837988826816, 0.6089385474860335, 0.7821229050279329, 0.6201117318435754, 0.7653631284916201]\n",
      "K-Fold Score: 0.7547486033519553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joshua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "model = LinearSVC()\n",
    "for train_index, test_index in cv.split(x,y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
    "    score = return_score(model,x_train, x_test, y_train, y_test)\n",
    "    scores.append(score)\n",
    "print(\"Accuracy score in each iteration: {}\".format(scores))\n",
    "print(\"K-Fold Score: {}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score in each iteration: [0.8268156424581006, 0.8491620111731844, 0.8379888268156425, 0.770949720670391, 0.8156424581005587, 0.7597765363128491, 0.7597765363128491, 0.6871508379888268, 0.7039106145251397, 0.6815642458100558, 0.8044692737430168, 0.8156424581005587, 0.7597765363128491, 0.770949720670391, 0.7597765363128491, 0.7150837988826816, 0.6089385474860335, 0.7821229050279329, 0.6201117318435754, 0.7653631284916201, 0.7150837988826816, 0.6871508379888268, 0.6871508379888268, 0.7150837988826816, 0.6256983240223464]\n",
      "K-Fold Score: 0.7410055865921786\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC()\n",
    "for train_index, test_index in cv.split(x,y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
    "    score = return_score(model,x_train, x_test, y_train, y_test)\n",
    "    scores.append(score)\n",
    "print(\"Accuracy score in each iteration: {}\".format(scores))\n",
    "print(\"K-Fold Score: {}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score in each iteration: [0.8268156424581006, 0.8491620111731844, 0.8379888268156425, 0.770949720670391, 0.8156424581005587, 0.7597765363128491, 0.7597765363128491, 0.6871508379888268, 0.7039106145251397, 0.6815642458100558, 0.8044692737430168, 0.8156424581005587, 0.7597765363128491, 0.770949720670391, 0.7597765363128491, 0.7150837988826816, 0.6089385474860335, 0.7821229050279329, 0.6201117318435754, 0.7653631284916201, 0.7150837988826816, 0.6871508379888268, 0.6871508379888268, 0.7150837988826816, 0.6256983240223464, 0.7430167597765364, 0.7597765363128491, 0.7653631284916201, 0.8379888268156425, 0.8715083798882681]\n",
      "K-Fold Score: 0.7500931098696462\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "for train_index, test_index in cv.split(x,y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
    "    score = return_score(model,x_train, x_test, y_train, y_test)\n",
    "    scores.append(score)\n",
    "print(\"Accuracy score in each iteration: {}\".format(scores))\n",
    "print(\"K-Fold Score: {}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score in each iteration: [0.8268156424581006, 0.8491620111731844, 0.8379888268156425, 0.770949720670391, 0.8156424581005587, 0.7597765363128491, 0.7597765363128491, 0.6871508379888268, 0.7039106145251397, 0.6815642458100558, 0.8044692737430168, 0.8156424581005587, 0.7597765363128491, 0.770949720670391, 0.7597765363128491, 0.7150837988826816, 0.6089385474860335, 0.7821229050279329, 0.6201117318435754, 0.7653631284916201, 0.7150837988826816, 0.6871508379888268, 0.6871508379888268, 0.7150837988826816, 0.6256983240223464, 0.7430167597765364, 0.7597765363128491, 0.7653631284916201, 0.8379888268156425, 0.8715083798882681, 0.8268156424581006, 0.8044692737430168, 0.8044692737430168, 0.8491620111731844, 0.8491620111731844]\n",
      "K-Fold Score: 0.7610534716679968\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "for train_index, test_index in cv.split(x,y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
    "    score = return_score(model,x_train, x_test, y_train, y_test)\n",
    "    scores.append(score)\n",
    "print(\"Accuracy score in each iteration: {}\".format(scores))\n",
    "print(\"K-Fold Score: {}\".format(np.mean(scores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2daf0c181f587f0623bb4008fadbfe2739bbcebb31a24c63b968834edd7b4dc0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
